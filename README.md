
# Fine Tune LLMS effectively using PEFT(LORA)

Problem with training large language models and fine tuning them is The llmâ€™s are computationally very expensive and Their file size becomes very huge as they have billions of parameters with huge weights.

To solve these problems the idea of parameter efficient fine tuning comes into picture.PEFT uses a variety of fine tuning techniques but for now lets concentrate on LORA (Low Rank Adaptation Of Large Language Models)

feel free to refer this article for more information.
https://medium.com/@venkata_sai/unleashing-the-potential-of-peft-parameter-efficient-fine-tuning-in-training-large-language-b7a87e8a4eb9

